# -*- coding: utf-8 -*-
"""Copy of fmorginal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qXwhQs60Y6ABvDY_CSw2cpi2Ap-V7GhD
"""

from keras.models import Model
from keras.layers import Dense, Input
from keras.datasets import mnist
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import warnings
warnings.filterwarnings('ignore')

import tensorflow as tf
(x_trainf, _), (x_testf, _) = tf.keras.datasets.fashion_mnist.load_data()
x_trainf = x_trainf.astype('float32') / 255.
x_testf = x_testf.astype('float32') / 255.
x_trainf = x_trainf.reshape((len(x_trainf), np.prod(x_trainf.shape[1:])))
x_testf = x_testf.reshape((len(x_testf), np.prod(x_testf.shape[1:])))
print(x_trainf.shape)
print(x_testf.shape)

batch_sizef = 128
nb_epochf = 10
img_rowsf, img_colsf = 28, 28
nb_visiblef = img_rowsf * img_colsf
nb_hidden1f = 392
nb_hidden2f = 196

input_imgf = Input(shape=(nb_visiblef,))
encoded1f = Dense(nb_hidden1f, activation='relu')(input_imgf)
encoded2f = Dense(nb_hidden2f, activation='relu')(encoded1f)
decodedf = Dense(nb_visiblef, activation='sigmoid')(encoded2f)

autoencoderf = Model(input=input_imgf, output=decodedf)
autoencoderf.compile(optimizer='adadelta', loss='binary_crossentropy')
autoencoderf.summary()

autoencoderf.fit(x_trainf, x_trainf, epochs=nb_epochf, batch_size=batch_sizef, shuffle=True, verbose=1,
                validation_data=(x_testf, x_testf))

decoded_imgsf = autoencoderf.predict(x_testf)
n=15
plt.figure(figsize=(20, 4))
for i in range(n):
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_testf[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgsf[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

w = []
for layer in autoencoderf.layers:
    weights = layer.get_weights()
    w.append(weights)

layer1 = np.array(w[2][0])
print("Shape of Hidden Layer",layer1.shape)
print("Visualization of Hidden Layer")
fig=plt.figure(figsize=(12, 12))
columns = 2
rows = 1
for i in range(1, columns*rows +1):
    fig.add_subplot(rows, columns, i)
    plt.imshow(layer1[:,i-1].reshape(28,14),cmap='gray')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# % matplotlib inline
# A thing of beauty is a joy forever
import seaborn as sns
sns.set()
sns.set_palette("husl")

C = 10 # We need 4 clusters
m = 2 # This is arbitrary
w = []
for layer in autoencoderf.layers:
    weights = layer.get_weights()
    w.append(weights)

data = np.array(w[2][0])

def initialize_memberships(data, C):
    """Returns an NxC array of randomly assigned memberships."""
    N, _ = data.shape
    # Create a random array of shape (N, C)
    mems = np.random.random((N, C))   
    # Divide by the sum along axis 1 so that the numbers sum up to 1
    return mems / mems.sum(axis=1, keepdims=True)

def update_centres(data, memberships, m):
    """Returns new centres from given data points and memberships."""
    # Notice the equation for cj.
    # Isn't that plain old matrix multiplication?
    mems = memberships ** m
    # We first calculate  δij^m / ∑ δij^m as weights
    weights = mems / mems.sum(axis=0)
    # And then do the matrix multiplication
    return np.dot(weights.T, data)

def get_distances(data, centres):
    """Returns distances between each point and every centre."""
    # Remember Life without Loops? Let's build upon it.
    # Please read through the numpy docs on broadcasting,
    # particularly the last example about outer addition
    # https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html
    return np.sqrt(((data[:, None] - centres)**2).sum(axis=2))

def update_memberships(data, centres, m):
    """Update the memberships according to new centres."""
    # You might have got it by now. Broadcasting rocks!
    dist = get_distances(data, centres)
    dist = dist[:, None, :] / dist[:, :, None]
    dist = dist ** (2/(m-1))
    return 1/dist.sum(axis=1)

def fcmeans(data, C, m):
    """The Fuzzy C-Means algorithm."""
    memberships = initialize_memberships(data, C)
    initial_centres = update_centres(data, memberships, m)
    old_memberships = np.zeros_like(memberships)
    while not np.allclose(memberships, old_memberships):
        old_memberships = memberships
        centres = update_centres(data, memberships, m)
        memberships = update_memberships(data, centres, m)
    return memberships, centres, initial_centres

C = 10
m = 2
#data = np.load('data/acc.npy')

memberships, centres, initial_centres = fcmeans(data, C, m)

plt.subplot(121)
plt.scatter(data[:, 0], data[:, 1])
plt.scatter(initial_centres[:, 0], initial_centres[:, 1], c='r', s=100)
plt.subplot(122)
plt.scatter(data[:, 0], data[:, 1], c=memberships.argmax(axis=1))
plt.scatter(centres[:, 0], centres[:, 1], c='r', s=100)
plt.show()